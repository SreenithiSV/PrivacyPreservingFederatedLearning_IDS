{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2830743, 79)\n",
      "Columns: [' Destination Port', ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', ' Fwd Packet Length Max', ' Fwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Packet Length Std', 'Bwd Packet Length Max', ' Bwd Packet Length Min', ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size', ' Avg Fwd Segment Size', ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward', ' Init_Win_bytes_backward', ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min', ' Label']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "monday_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Monday-WorkingHours.pcap_ISCX.csv\"\n",
    "tuesday_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Tuesday-WorkingHours.pcap_ISCX.csv\"\n",
    "wednesday_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Wednesday-workingHours.pcap_ISCX.csv\"\n",
    "thursday_m_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
    "thursday_a_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
    "friday_dos_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
    "friday_portscan_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"\n",
    "friday_morning_path = r\"C:\\Users\\SREENITHI\\fed_learning_cicids2017\\MachineLearningCSV\\MachineLearningCVE\\Friday-WorkingHours-Morning.pcap_ISCX.csv\"\n",
    "\n",
    "monday = pd.read_csv(monday_path)\n",
    "tuesday = pd.read_csv(tuesday_path)\n",
    "wednesday = pd.read_csv(wednesday_path)\n",
    "thursday_m = pd.read_csv(thursday_m_path)\n",
    "thursday_a = pd.read_csv(thursday_a_path)\n",
    "friday_dos = pd.read_csv(friday_dos_path)\n",
    "friday_portscan = pd.read_csv(friday_portscan_path)\n",
    "friday_morning = pd.read_csv(friday_morning_path)\n",
    "full_df = pd.concat([monday, tuesday, wednesday, thursday_m, thursday_a, \n",
    "                  friday_dos, friday_portscan, friday_morning], ignore_index=True)\n",
    "\n",
    "print(\"Dataset shape:\", full_df.shape)\n",
    "print(\"Columns:\", full_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after label encoding + numeric filter: (2830743, 79)\n",
      "Final dataset shape: (2827876, 78) Labels shape: (2827876,)\n",
      "Train: (2262300, 78) Test: (565576,)\n",
      "Client 0 label counts: {0: 290522, 1: 71446}\n",
      "Client 1 label counts: {0: 193795, 1: 71263}\n",
      "Client 2 label counts: {0: 290619, 1: 23856}\n",
      "Client 3 label counts: {0: 290885, 1: 71083}\n",
      "Client 4 label counts: {0: 290797, 1: 47424}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode labels (BENIGN = 0, ATTACK = 1)\n",
    "def encode_labels(df):\n",
    "    le = LabelEncoder()\n",
    "    df[' Label'] = le.fit_transform(df[' Label'])\n",
    "    df[' Label'] = df[' Label'].apply(\n",
    "        lambda x: 0 if le.classes_[x] == 'BENIGN' else 1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "full_df = encode_labels(full_df)\n",
    "\n",
    "# Keep only numeric columns\n",
    "full_df = full_df.select_dtypes(include=[np.number])\n",
    "print(\"Dataset after label encoding + numeric filter:\", full_df.shape)\n",
    "\n",
    "# Features & Labels + Cleaning\n",
    "X = full_df.drop(' Label', axis=1).values\n",
    "y = full_df[' Label'].values\n",
    "\n",
    "# Replace inf → nan\n",
    "X[np.isinf(X)] = np.nan\n",
    "\n",
    "# Drop rows with NaN\n",
    "mask = ~np.isnan(X).any(axis=1)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "print(\"Final dataset shape:\", X.shape, \"Labels shape:\", y.shape)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", y_test.shape)\n",
    "\n",
    "\n",
    "#  Config for Federated run\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "NUM_EPOCHS = 5           # federated rounds\n",
    "LOCAL_EPOCHS = 1         # local epochs per round\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FED_NONIID = True        # False = IID split, True = Non-IID split\n",
    "USE_BOOTSTRAP = True     # Toggle bootstrapping for client sampling\n",
    "BOOTSTRAP_RATIO = 0.8    # Resample % of each client dataset\n",
    "\n",
    "#  Partitioning Functions\n",
    "def iid_partition(X, y, num_clients=NUM_CLIENTS, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(X))\n",
    "    rng.shuffle(idx)\n",
    "    splits = np.array_split(idx, num_clients)\n",
    "    return [(X[s], y[s]) for s in splits]\n",
    "\n",
    "def noniid_partition(X, y, num_clients=NUM_CLIENTS, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    idx_0 = np.where(y == 0)[0]   # BENIGN\n",
    "    idx_1 = np.where(y == 1)[0]   # ATTACK\n",
    "    np.random.shuffle(idx_0); np.random.shuffle(idx_1)\n",
    "\n",
    "    size_0 = len(idx_0) // num_clients\n",
    "    size_1 = len(idx_1) // num_clients\n",
    "\n",
    "    clients_data = []\n",
    "    for i in range(num_clients):\n",
    "        if i % 2 == 0:  # even clients → mostly BENIGN\n",
    "            main_idx = idx_0[size_0*i : size_0*(i+1)]\n",
    "            other_idx_end = min(len(idx_1), size_1*(i//3 + 1))\n",
    "            other_idx = idx_1[size_1*i//3 : other_idx_end]\n",
    "        else:           # odd clients → mostly ATTACK\n",
    "            main_idx = idx_1[size_1*i : size_1*(i+1)]\n",
    "            other_idx_end = min(len(idx_0), size_0*(i//3 + 1))\n",
    "            other_idx = idx_0[size_0*i//3 : other_idx_end]\n",
    "\n",
    "        indices = np.concatenate((main_idx, other_idx))\n",
    "        np.random.shuffle(indices)\n",
    "        clients_data.append((X[indices], y[indices]))\n",
    "    return clients_data\n",
    "\n",
    "# Bootstrapping Function\n",
    "\n",
    "def bootstrap_client_data(clients_data, ratio=BOOTSTRAP_RATIO, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    boot_clients = []\n",
    "    for Xc, yc in clients_data:\n",
    "        n_samples = int(len(Xc) * ratio)\n",
    "        idx = rng.choice(len(Xc), n_samples, replace=True)\n",
    "        boot_clients.append((Xc[idx], yc[idx]))\n",
    "    return boot_clients\n",
    "\n",
    "#  Get Clients Data\n",
    "clients_data = (\n",
    "    noniid_partition(X_train, y_train, NUM_CLIENTS)\n",
    "    if FED_NONIID else\n",
    "    iid_partition(X_train, y_train, NUM_CLIENTS)\n",
    ")\n",
    "\n",
    "if USE_BOOTSTRAP:\n",
    "    clients_data = bootstrap_client_data(clients_data)\n",
    "\n",
    "# Quick check: print label distribution\n",
    "for i, (Xc, yc) in enumerate(clients_data):\n",
    "    u, c = np.unique(yc, return_counts=True)\n",
    "    print(f\"Client {i} label counts:\", dict(zip(u, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c590e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clients_data(\n",
    "    X, y,\n",
    "    num_clients=5,\n",
    "    noniid=True,\n",
    "    use_bootstrap=True,\n",
    "    bootstrap_ratio=0.8,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"Prepare client datasets for federated training.\"\"\"\n",
    "    # Partitioning\n",
    "    if noniid:\n",
    "        clients_data = noniid_partition(X, y, num_clients, seed)\n",
    "    else:\n",
    "        clients_data = iid_partition(X, y, num_clients, seed)\n",
    "\n",
    "    print(\"\\n--- Before Bootstrapping ---\")\n",
    "    for i, (Xc, yc) in enumerate(clients_data):\n",
    "        print(f\" Client {i}: {Xc.shape[0]} samples, labels={dict(zip(*np.unique(yc, return_counts=True)))}\")\n",
    "\n",
    "    # Optional Bootstrapping\n",
    "    if use_bootstrap:\n",
    "        clients_data = bootstrap_client_data(clients_data, bootstrap_ratio, seed)\n",
    "        print(\"\\n--- After Bootstrapping ---\")\n",
    "        for i, (Xc, yc) in enumerate(clients_data):\n",
    "            print(f\" Client {i}: {Xc.shape[0]} samples, labels={dict(zip(*np.unique(yc, return_counts=True)))}\")\n",
    "\n",
    "    return clients_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffaf66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Before Bootstrapping ---\n",
      " Client 0: 452460 samples, labels={0: 363411, 1: 89049}\n",
      " Client 1: 331323 samples, labels={0: 242274, 1: 89049}\n",
      " Client 2: 393094 samples, labels={0: 363411, 1: 29683}\n",
      " Client 3: 452460 samples, labels={0: 363411, 1: 89049}\n",
      " Client 4: 422777 samples, labels={0: 363411, 1: 59366}\n",
      "\n",
      "--- After Bootstrapping ---\n",
      " Client 0: 361968 samples, labels={0: 290522, 1: 71446}\n",
      " Client 1: 265058 samples, labels={0: 193795, 1: 71263}\n",
      " Client 2: 314475 samples, labels={0: 290619, 1: 23856}\n",
      " Client 3: 361968 samples, labels={0: 290885, 1: 71083}\n",
      " Client 4: 338221 samples, labels={0: 290797, 1: 47424}\n",
      "\n",
      "--- Training RF on Client 0 ---\n",
      "\n",
      "--- Training RF on Client 1 ---\n",
      "\n",
      "--- Training RF on Client 2 ---\n",
      "\n",
      "--- Training RF on Client 3 ---\n",
      "\n",
      "--- Training RF on Client 4 ---\n",
      "\n",
      "=== Federated Random Forest Results ===\n",
      "Accuracy: 0.9987\n",
      "Precision: 0.9973\n",
      "Recall: 0.9959\n",
      "F1-score: 0.9966\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Run federated RF\n",
    "clients_data = get_clients_data(\n",
    "    X_train, y_train,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    noniid=True,\n",
    "    use_bootstrap=True\n",
    ")\n",
    "\n",
    "final_preds = None  # To hold results\n",
    "\n",
    "def train_local_rf(X_train, y_train, n_estimators=50, random_state=42):\n",
    "    \"\"\"Train Random Forest locally on client data.\"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def federated_random_forest(clients_data, X_test, y_test):\n",
    "    client_models = []\n",
    "    for i, (Xc, yc) in enumerate(clients_data):\n",
    "        print(f\"\\n--- Training RF on Client {i} ---\")\n",
    "        model = train_local_rf(Xc, yc)\n",
    "        client_models.append(model)\n",
    "\n",
    "    # Collect predictions from each client model\n",
    "    preds = np.array([m.predict(X_test) for m in client_models])\n",
    "    \n",
    "    # Majority vote (0/1 classification)\n",
    "    final_preds = (preds.sum(axis=0) >= (len(client_models) / 2)).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, final_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, final_preds, average=\"binary\"\n",
    "    )\n",
    "    print(\"\\n=== Federated Random Forest Results ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    return final_preds\n",
    "\n",
    "final_preds = federated_random_forest(clients_data, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524321ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Decision Tree on Client 0 ---\n",
      "\n",
      "--- Training Decision Tree on Client 1 ---\n",
      "\n",
      "--- Training Decision Tree on Client 2 ---\n",
      "\n",
      "--- Training Decision Tree on Client 3 ---\n",
      "\n",
      "--- Training Decision Tree on Client 4 ---\n",
      "\n",
      "=== Federated Decision Tree Results ===\n",
      "Accuracy: 0.9991\n",
      "Precision: 0.9969\n",
      "Recall: 0.9984\n",
      "F1-score: 0.9976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Train Local Decision Tree\n",
    "def train_local_dt(X_train, y_train, max_depth=None, random_state=42):\n",
    "    \"\"\"Train Decision Tree locally on client data.\"\"\"\n",
    "    model = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Federated Decision Tree\n",
    "def federated_decision_tree(clients_data, X_test, y_test):\n",
    "    client_models = []\n",
    "    for i, (Xc, yc) in enumerate(clients_data):\n",
    "        print(f\"\\n--- Training Decision Tree on Client {i} ---\")\n",
    "        model = train_local_dt(Xc, yc)\n",
    "        client_models.append(model)\n",
    "\n",
    "    # Collect predictions from each client model\n",
    "    preds = np.array([m.predict(X_test) for m in client_models])\n",
    "    \n",
    "    # Majority vote (0/1 classification)\n",
    "    final_preds = (preds.sum(axis=0) >= (len(client_models) / 2)).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, final_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, final_preds, average=\"binary\"\n",
    "    )\n",
    "    print(\"\\n=== Federated Decision Tree Results ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    return final_preds\n",
    "\n",
    "\n",
    "final_preds = federated_decision_tree(clients_data, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70a1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape (before preprocessing): (1000, 79)\n",
      "Subset after preprocessing: (1000, 78) (1000,)\n",
      "\n",
      "=== Gradient Boosting Results (1000-sample subset) ===\n",
      "Accuracy:  0.9900\n",
      "Precision: 0.9756\n",
      "Recall:    0.9756\n",
      "F1-score:  0.9756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample equal data from each dataset \n",
    "sample_size = 1000 // 8  # =125\n",
    "monday_sample = monday.sample(n=sample_size, random_state=42)\n",
    "tuesday_sample = tuesday.sample(n=sample_size, random_state=42)\n",
    "wednesday_sample = wednesday.sample(n=sample_size, random_state=42)\n",
    "thursday_m_sample = thursday_m.sample(n=sample_size, random_state=42)\n",
    "thursday_a_sample = thursday_a.sample(n=sample_size, random_state=42)\n",
    "friday_dos_sample = friday_dos.sample(n=sample_size, random_state=42)\n",
    "friday_portscan_sample = friday_portscan.sample(n=sample_size, random_state=42)\n",
    "friday_morning_sample = friday_morning.sample(n=sample_size, random_state=42)\n",
    "\n",
    "small_df = pd.concat([\n",
    "    monday_sample, tuesday_sample, wednesday_sample,\n",
    "    thursday_m_sample, thursday_a_sample,\n",
    "    friday_dos_sample, friday_portscan_sample, friday_morning_sample\n",
    "], ignore_index=True)\n",
    "\n",
    "small_df = small_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(\"Subset shape (before preprocessing):\", small_df.shape)\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_pipeline(df):\n",
    "    # Encode labels\n",
    "    df = encode_labels(df)\n",
    "\n",
    "    # Keep only numeric\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Features & Labels\n",
    "    X = df.drop(' Label', axis=1).values\n",
    "    y = df[' Label'].values\n",
    "\n",
    "    # Replace inf with NaN, drop NaNs\n",
    "    X[np.isinf(X)] = np.nan\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X, y = X[mask], y[mask]\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_small, y_small = preprocess_pipeline(small_df)\n",
    "print(\"Subset after preprocessing:\", X_small.shape, y_small.shape)\n",
    "\n",
    "#  Train/Test Split ---\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(\n",
    "    X_small, y_small, test_size=0.2, stratify=y_small, random_state=42\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# --- Gradient Boosting ---\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(Xs_train, ys_train)\n",
    "ys_pred = gb_model.predict(Xs_test)\n",
    "\n",
    "acc = accuracy_score(ys_test, ys_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    ys_test, ys_pred, average=\"binary\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Gradient Boosting Results (1000-sample subset) ===\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "final_preds = ys_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
