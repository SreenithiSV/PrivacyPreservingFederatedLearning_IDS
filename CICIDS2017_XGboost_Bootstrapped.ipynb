{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Imports\n",
        "# ================================\n",
        "import os, glob, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ML/Preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Torch (for later models)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ================================\n",
        "# 2. Mount Google Drive\n",
        "# ================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset path\n",
        "DATA_PATH = \"/content/drive/MyDrive/datasets/CICIDS2017/\"\n",
        "\n",
        "# ================================\n",
        "# 3. Load all CSV files\n",
        "# ================================\n",
        "csv_files = glob.glob(DATA_PATH + \"*.csv\")\n",
        "print(\"Found files:\", len(csv_files))\n",
        "\n",
        "dfs = []\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs.append(df)\n",
        "\n",
        "full_df = pd.concat(dfs, ignore_index=True)\n",
        "print(\"Full dataset shape:\", full_df.shape)\n",
        "\n",
        "# ================================\n",
        "# Encode labels (BENIGN = 0, ATTACK = 1)\n",
        "# ================================\n",
        "def encode_labels(df):\n",
        "    le = LabelEncoder()\n",
        "    df[' Label'] = le.fit_transform(df[' Label'])\n",
        "    df[' Label'] = df[' Label'].apply(\n",
        "        lambda x: 0 if le.classes_[x] == 'BENIGN' else 1\n",
        "    )\n",
        "    return df\n",
        "\n",
        "full_df = encode_labels(full_df)\n",
        "\n",
        "# Keep only numeric columns\n",
        "full_df = full_df.select_dtypes(include=[np.number])\n",
        "print(\"Dataset after label encoding + numeric filter:\", full_df.shape)\n",
        "\n",
        "# ================================\n",
        "# Features & Labels + Cleaning\n",
        "# ================================\n",
        "X = full_df.drop(' Label', axis=1).values\n",
        "y = full_df[' Label'].values\n",
        "\n",
        "# Replace inf → nan\n",
        "X[np.isinf(X)] = np.nan\n",
        "\n",
        "# Drop rows with NaN\n",
        "mask = ~np.isnan(X).any(axis=1)\n",
        "X, y = X[mask], y[mask]\n",
        "\n",
        "print(\"Final dataset shape:\", X.shape, \"Labels shape:\", y.shape)\n",
        "\n",
        "# ================================\n",
        "# Standardize\n",
        "# ================================\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Test:\", y_test.shape)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. Config for Federated run\n",
        "# ================================\n",
        "NUM_CLIENTS = 5\n",
        "NUM_EPOCHS = 5           # federated rounds\n",
        "LOCAL_EPOCHS = 1         # local epochs per round\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "FED_NONIID = True        # False = IID split, True = Non-IID split\n",
        "USE_BOOTSTRAP = True     # Toggle bootstrapping for client sampling\n",
        "BOOTSTRAP_RATIO = 0.8    # Resample % of each client dataset\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5. Partitioning Functions\n",
        "# ================================\n",
        "def iid_partition(X, y, num_clients=NUM_CLIENTS, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(X))\n",
        "    rng.shuffle(idx)\n",
        "    splits = np.array_split(idx, num_clients)\n",
        "    return [(X[s], y[s]) for s in splits]\n",
        "\n",
        "def noniid_partition(X, y, num_clients=NUM_CLIENTS, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    idx_0 = np.where(y == 0)[0]   # BENIGN\n",
        "    idx_1 = np.where(y == 1)[0]   # ATTACK\n",
        "    np.random.shuffle(idx_0); np.random.shuffle(idx_1)\n",
        "\n",
        "    size_0 = len(idx_0) // num_clients\n",
        "    size_1 = len(idx_1) // num_clients\n",
        "\n",
        "    clients_data = []\n",
        "    for i in range(num_clients):\n",
        "        if i % 2 == 0:  # even clients → mostly BENIGN\n",
        "            main_idx = idx_0[size_0*i : size_0*(i+1)]\n",
        "            other_idx_end = min(len(idx_1), size_1*(i//3 + 1))\n",
        "            other_idx = idx_1[size_1*i//3 : other_idx_end]\n",
        "        else:           # odd clients → mostly ATTACK\n",
        "            main_idx = idx_1[size_1*i : size_1*(i+1)]\n",
        "            other_idx_end = min(len(idx_0), size_0*(i//3 + 1))\n",
        "            other_idx = idx_0[size_0*i//3 : other_idx_end]\n",
        "\n",
        "        indices = np.concatenate((main_idx, other_idx))\n",
        "        np.random.shuffle(indices)\n",
        "        clients_data.append((X[indices], y[indices]))\n",
        "    return clients_data\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. Bootstrapping Function\n",
        "# ================================\n",
        "def bootstrap_client_data(clients_data, ratio=BOOTSTRAP_RATIO, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    boot_clients = []\n",
        "    for Xc, yc in clients_data:\n",
        "        n_samples = int(len(Xc) * ratio)\n",
        "        idx = rng.choice(len(Xc), n_samples, replace=True)\n",
        "        boot_clients.append((Xc[idx], yc[idx]))\n",
        "    return boot_clients\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 7. Get Clients Data\n",
        "# ================================\n",
        "clients_data = (\n",
        "    noniid_partition(X_train, y_train, NUM_CLIENTS)\n",
        "    if FED_NONIID else\n",
        "    iid_partition(X_train, y_train, NUM_CLIENTS)\n",
        ")\n",
        "\n",
        "if USE_BOOTSTRAP:\n",
        "    clients_data = bootstrap_client_data(clients_data)\n",
        "\n",
        "# Quick check: print label distribution\n",
        "for i, (Xc, yc) in enumerate(clients_data):\n",
        "    u, c = np.unique(yc, return_counts=True)\n",
        "    print(f\"Client {i} label counts:\", dict(zip(u, c)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjIveu_jdajf",
        "outputId": "45ce4fcb-b01a-466c-d675-6ec8c6989163"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found files: 8\n",
            "Full dataset shape: (2830743, 79)\n",
            "Dataset after label encoding + numeric filter: (2830743, 79)\n",
            "Final dataset shape: (2827876, 78) Labels shape: (2827876,)\n",
            "Train: (2262300, 78) Test: (565576,)\n",
            "Client 0 label counts: {np.int64(0): np.int64(290522), np.int64(1): np.int64(71446)}\n",
            "Client 1 label counts: {np.int64(0): np.int64(193795), np.int64(1): np.int64(71263)}\n",
            "Client 2 label counts: {np.int64(0): np.int64(290619), np.int64(1): np.int64(23856)}\n",
            "Client 3 label counts: {np.int64(0): np.int64(290885), np.int64(1): np.int64(71083)}\n",
            "Client 4 label counts: {np.int64(0): np.int64(290797), np.int64(1): np.int64(47424)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clients_data(\n",
        "    X, y,\n",
        "    num_clients=5,\n",
        "    noniid=True,\n",
        "    use_bootstrap=True,\n",
        "    bootstrap_ratio=0.8,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"Prepare client datasets for federated training.\"\"\"\n",
        "    # Partitioning\n",
        "    if noniid:\n",
        "        clients_data = noniid_partition(X, y, num_clients, seed)\n",
        "    else:\n",
        "        clients_data = iid_partition(X, y, num_clients, seed)\n",
        "\n",
        "    print(\"\\n--- Before Bootstrapping ---\")\n",
        "    for i, (Xc, yc) in enumerate(clients_data):\n",
        "        print(f\" Client {i}: {Xc.shape[0]} samples, labels={dict(zip(*np.unique(yc, return_counts=True)))}\")\n",
        "\n",
        "    # Optional Bootstrapping\n",
        "    if use_bootstrap:\n",
        "        clients_data = bootstrap_client_data(clients_data, bootstrap_ratio, seed)\n",
        "        print(\"\\n--- After Bootstrapping ---\")\n",
        "        for i, (Xc, yc) in enumerate(clients_data):\n",
        "            print(f\" Client {i}: {Xc.shape[0]} samples, labels={dict(zip(*np.unique(yc, return_counts=True)))}\")\n",
        "\n",
        "    return clients_data"
      ],
      "metadata": {
        "id": "U1sj1-yaeaKg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# XGBoost Federated Training\n",
        "# ================================\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def federated_xgboost(clients_data, X_test, y_test, rounds=3, params=None):\n",
        "    \"\"\"\n",
        "    Simulated federated XGBoost training.\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"max_depth\": 6,\n",
        "            \"n_estimators\": 50,\n",
        "            \"subsample\": 0.8,\n",
        "            \"colsample_bytree\": 0.8,\n",
        "            \"tree_method\": \"hist\"\n",
        "        }\n",
        "\n",
        "    global_model = None\n",
        "\n",
        "    for rnd in range(rounds):\n",
        "        print(f\"\\n===== Federated Round {rnd+1} =====\")\n",
        "        client_models = []\n",
        "\n",
        "        # Train local models\n",
        "        for i, (Xc, yc) in enumerate(clients_data):\n",
        "            clf = xgb.XGBClassifier(**params, use_label_encoder=False, verbosity=0)\n",
        "            clf.fit(Xc, yc)\n",
        "            client_models.append(clf)\n",
        "            print(f\" Client {i} trained.\")\n",
        "\n",
        "        # Aggregate (simple averaging of trees/boosters is tricky → we’ll average predictions)\n",
        "        # Collect predictions from all client models and average logits\n",
        "        test_preds = np.zeros(len(y_test))\n",
        "        for clf in client_models:\n",
        "            test_preds += clf.predict_proba(X_test)[:,1]\n",
        "        test_preds /= len(client_models)\n",
        "\n",
        "        # Threshold for classification\n",
        "        y_pred = (test_preds >= 0.5).astype(int)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\" Global Model Accuracy (Round {rnd+1}): {acc:.4f}\")\n",
        "\n",
        "        # Pick best performing client model as proxy for global (optional simplification)\n",
        "        global_model = client_models[0]\n",
        "\n",
        "    return global_model"
      ],
      "metadata": {
        "id": "cMhnxyi3kxNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_data = get_clients_data(\n",
        "    X_train, y_train,\n",
        "    num_clients=5,\n",
        "    noniid=True,\n",
        "    use_bootstrap=True,\n",
        "    bootstrap_ratio=0.2\n",
        ")\n",
        "global_model = federated_xgboost(clients_data, X_test, y_test, rounds=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnWob934hm4y",
        "outputId": "305c62d2-c06f-4bbc-9466-08619014541f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Before Bootstrapping ---\n",
            " Client 0: 452460 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(89049)}\n",
            " Client 1: 331323 samples, labels={np.int64(0): np.int64(242274), np.int64(1): np.int64(89049)}\n",
            " Client 2: 393094 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(29683)}\n",
            " Client 3: 452460 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(89049)}\n",
            " Client 4: 422777 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(59366)}\n",
            "\n",
            "--- After Bootstrapping ---\n",
            " Client 0: 90492 samples, labels={np.int64(0): np.int64(72811), np.int64(1): np.int64(17681)}\n",
            " Client 1: 66264 samples, labels={np.int64(0): np.int64(48442), np.int64(1): np.int64(17822)}\n",
            " Client 2: 78618 samples, labels={np.int64(0): np.int64(72663), np.int64(1): np.int64(5955)}\n",
            " Client 3: 90492 samples, labels={np.int64(0): np.int64(72714), np.int64(1): np.int64(17778)}\n",
            " Client 4: 84555 samples, labels={np.int64(0): np.int64(72749), np.int64(1): np.int64(11806)}\n",
            "\n",
            "===== Federated Round 1 =====\n",
            " Client 0 trained.\n",
            " Client 1 trained.\n",
            " Client 2 trained.\n",
            " Client 3 trained.\n",
            " Client 4 trained.\n",
            " Global Model Accuracy (Round 1): 0.9975\n",
            "\n",
            "===== Federated Round 2 =====\n",
            " Client 0 trained.\n",
            " Client 1 trained.\n",
            " Client 2 trained.\n",
            " Client 3 trained.\n",
            " Client 4 trained.\n",
            " Global Model Accuracy (Round 2): 0.9975\n",
            "\n",
            "===== Federated Round 3 =====\n",
            " Client 0 trained.\n",
            " Client 1 trained.\n",
            " Client 2 trained.\n",
            " Client 3 trained.\n",
            " Client 4 trained.\n",
            " Global Model Accuracy (Round 3): 0.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clients_data = get_clients_data(\n",
        "    X_train, y_train,\n",
        "    num_clients=5,\n",
        "    noniid=True,\n",
        "    use_bootstrap=True,\n",
        "    bootstrap_ratio=0.8\n",
        ")\n",
        "global_model = federated_xgboost(clients_data, X_test, y_test, rounds=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK_baImyftSs",
        "outputId": "53ac6204-4937-4f17-f747-e641e931c8ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Before Bootstrapping ---\n",
            " Client 0: 452460 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(89049)}\n",
            " Client 1: 331323 samples, labels={np.int64(0): np.int64(242274), np.int64(1): np.int64(89049)}\n",
            " Client 2: 393094 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(29683)}\n",
            " Client 3: 452460 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(89049)}\n",
            " Client 4: 422777 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(59366)}\n",
            "\n",
            "--- After Bootstrapping ---\n",
            " Client 0: 361968 samples, labels={np.int64(0): np.int64(290522), np.int64(1): np.int64(71446)}\n",
            " Client 1: 265058 samples, labels={np.int64(0): np.int64(193795), np.int64(1): np.int64(71263)}\n",
            " Client 2: 314475 samples, labels={np.int64(0): np.int64(290619), np.int64(1): np.int64(23856)}\n",
            " Client 3: 361968 samples, labels={np.int64(0): np.int64(290885), np.int64(1): np.int64(71083)}\n",
            " Client 4: 338221 samples, labels={np.int64(0): np.int64(290797), np.int64(1): np.int64(47424)}\n",
            "\n",
            "===== Federated Round 1 =====\n",
            " Client 0 trained.\n",
            " Client 1 trained.\n",
            " Client 2 trained.\n",
            " Client 3 trained.\n",
            " Client 4 trained.\n",
            " Global Model Accuracy (Round 1): 0.9978\n",
            "\n",
            "===== Federated Round 2 =====\n",
            " Client 0 trained.\n",
            " Client 1 trained.\n",
            " Client 2 trained.\n",
            " Client 3 trained.\n",
            " Client 4 trained.\n",
            " Global Model Accuracy (Round 2): 0.9978\n",
            "\n",
            "===== Federated Round 3 =====\n",
            " Client 0 trained.\n",
            " Client 1 trained.\n",
            " Client 2 trained.\n",
            " Client 3 trained.\n",
            " Client 4 trained.\n",
            " Global Model Accuracy (Round 3): 0.9978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Ensemble wrapper for the \"global\" model =========\n",
        "class XGBEnsemble:\n",
        "    \"\"\"Wraps a list of trained XGBClassifier models and exposes predict/predict_proba.\"\"\"\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Average the positive-class probabilities across clients\n",
        "        import numpy as np\n",
        "        probs = np.zeros(X.shape[0], dtype=float)\n",
        "        for m in self.models:\n",
        "            probs += m.predict_proba(X)[:, 1]\n",
        "        probs /= len(self.models)  # <-- correct averaging (outside the loop)\n",
        "        # Return 2-column proba to mimic sklearn API\n",
        "        return np.vstack([1.0 - probs, probs]).T\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X)[:, 1] >= threshold).astype(int)\n",
        "\n",
        "# ========= Metrics helper =========\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
        "\n",
        "def eval_metrics(y_true, y_proba, threshold=0.5):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    # Guard AUC if one class missing in y_true\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_proba)\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    return {\"acc\": acc, \"auc\": auc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "\n",
        "# ========= Federated XGBoost (simulation) =========\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "def federated_xgboost(\n",
        "    clients_data,\n",
        "    X_test, y_test,\n",
        "    rounds=3,\n",
        "    params=None,\n",
        "    client_fraction=1.0,        # e.g., 0.6 for partial participation\n",
        "    rebootstrap=False,          # if True, re-sample (bootstrap) client data every round\n",
        "    bootstrap_ratio=0.8,        # ignored unless rebootstrap=True\n",
        "    bootstrap_seed=42,\n",
        "    verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Simulated federated XGBoost with non-IID clients.\n",
        "    - Aggregation: probability averaging (ensemble).\n",
        "    - Optional partial participation (client_fraction).\n",
        "    - Optional re-bootstrap per round (rebootstrap=True).\n",
        "    Returns:\n",
        "        global_ensemble (XGBEnsemble), history (list of metrics per round)\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"max_depth\": 6,\n",
        "            \"n_estimators\": 50,\n",
        "            \"subsample\": 0.8,\n",
        "            \"colsample_bytree\": 0.8,\n",
        "            \"tree_method\": \"hist\",\n",
        "        }\n",
        "\n",
        "    rng = np.random.default_rng(bootstrap_seed)\n",
        "    original_clients = clients_data  # keep a copy for per-round bootstrapping\n",
        "    history = []\n",
        "\n",
        "    for rnd in range(1, rounds + 1):\n",
        "        if verbose:\n",
        "            print(f\"\\n===== Federated Round {rnd} =====\")\n",
        "\n",
        "        # Optional: re-bootstrap client data every round to simulate drift/variability\n",
        "        if rebootstrap:\n",
        "            # expects your bootstrap_client_data(clients_data, ratio, seed) to exist\n",
        "            clients_data = bootstrap_client_data(original_clients, ratio=bootstrap_ratio, seed=rng.integers(0, 1_000_000))\n",
        "\n",
        "        # Partial client participation\n",
        "        num_clients = len(clients_data)\n",
        "        m = max(1, int(np.ceil(client_fraction * num_clients)))\n",
        "        selected = rng.choice(num_clients, size=m, replace=False)\n",
        "\n",
        "        client_models = []\n",
        "        for i in selected:\n",
        "            Xc, yc = clients_data[i]\n",
        "            clf = xgb.XGBClassifier(**params, use_label_encoder=False, verbosity=0)\n",
        "            clf.fit(Xc, yc)\n",
        "            client_models.append(clf)\n",
        "            if verbose:\n",
        "                print(f\"  Client {i} trained on {len(yc)} samples.\")\n",
        "\n",
        "        # Build global ensemble and evaluate\n",
        "        global_ensemble = XGBEnsemble(client_models)\n",
        "        test_proba = global_ensemble.predict_proba(X_test)[:, 1]\n",
        "        metrics = eval_metrics(y_test, test_proba, threshold=0.5)\n",
        "        history.append(metrics)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  Global (ensemble) — \"\n",
        "                  f\"ACC: {metrics['acc']:.4f} | AUC: {metrics['auc']:.4f} | \"\n",
        "                  f\"P: {metrics['precision']:.4f} | R: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
        "\n",
        "    # Return the final ensemble and the metrics history\n",
        "    return global_ensemble, history"
      ],
      "metadata": {
        "id": "iXyGU4Rkfq57"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_data = get_clients_data(\n",
        "    X_train, y_train,\n",
        "    num_clients=5,\n",
        "    noniid=True,\n",
        "    use_bootstrap=True,\n",
        "    bootstrap_ratio=0.5\n",
        ")\n",
        "global_model, hist = federated_xgboost(\n",
        "    clients_data,\n",
        "    X_test, y_test,\n",
        "    rounds=3,\n",
        "    client_fraction=1.0,    # or < 1.0 for partial participation\n",
        "    rebootstrap=True,       # set False to keep fixed client data across rounds\n",
        "    bootstrap_ratio=0.8,\n",
        "    bootstrap_seed=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Use the ensemble like a normal model:\n",
        "y_proba = global_model.predict_proba(X_test)[:,1]\n",
        "y_pred = global_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfsEzwVbkNUE",
        "outputId": "78e3e1e9-d13a-446e-95ac-ca2bed95ac4e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Before Bootstrapping ---\n",
            " Client 0: 452460 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(89049)}\n",
            " Client 1: 331323 samples, labels={np.int64(0): np.int64(242274), np.int64(1): np.int64(89049)}\n",
            " Client 2: 393094 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(29683)}\n",
            " Client 3: 452460 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(89049)}\n",
            " Client 4: 422777 samples, labels={np.int64(0): np.int64(363411), np.int64(1): np.int64(59366)}\n",
            "\n",
            "--- After Bootstrapping ---\n",
            " Client 0: 226230 samples, labels={np.int64(0): np.int64(181712), np.int64(1): np.int64(44518)}\n",
            " Client 1: 165661 samples, labels={np.int64(0): np.int64(120967), np.int64(1): np.int64(44694)}\n",
            " Client 2: 196547 samples, labels={np.int64(0): np.int64(181714), np.int64(1): np.int64(14833)}\n",
            " Client 3: 226230 samples, labels={np.int64(0): np.int64(181702), np.int64(1): np.int64(44528)}\n",
            " Client 4: 211388 samples, labels={np.int64(0): np.int64(181552), np.int64(1): np.int64(29836)}\n",
            "\n",
            "===== Federated Round 1 =====\n",
            "  Client 1 trained on 132528 samples.\n",
            "  Client 3 trained on 180984 samples.\n",
            "  Client 2 trained on 157237 samples.\n",
            "  Client 0 trained on 180984 samples.\n",
            "  Client 4 trained on 169110 samples.\n",
            "  Global (ensemble) — ACC: 0.9976 | AUC: 0.9998 | P: 0.9956 | R: 0.9920 | F1: 0.9938\n",
            "\n",
            "===== Federated Round 2 =====\n",
            "  Client 2 trained on 157237 samples.\n",
            "  Client 0 trained on 180984 samples.\n",
            "  Client 1 trained on 132528 samples.\n",
            "  Client 4 trained on 169110 samples.\n",
            "  Client 3 trained on 180984 samples.\n",
            "  Global (ensemble) — ACC: 0.9975 | AUC: 0.9998 | P: 0.9955 | R: 0.9919 | F1: 0.9937\n",
            "\n",
            "===== Federated Round 3 =====\n",
            "  Client 2 trained on 157237 samples.\n",
            "  Client 0 trained on 180984 samples.\n",
            "  Client 1 trained on 132528 samples.\n",
            "  Client 3 trained on 180984 samples.\n",
            "  Client 4 trained on 169110 samples.\n",
            "  Global (ensemble) — ACC: 0.9975 | AUC: 0.9998 | P: 0.9955 | R: 0.9916 | F1: 0.9935\n"
          ]
        }
      ]
    }
  ]
}