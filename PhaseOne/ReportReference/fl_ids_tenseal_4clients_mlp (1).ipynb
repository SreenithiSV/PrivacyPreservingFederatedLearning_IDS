{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AohgY_o3A87"
      },
      "source": [
        "# Federated IDS (4 clients) with TenSEAL (CKKS) encrypted weight aggregation\n",
        "\n"
      ],
      "id": "-AohgY_o3A87"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TenSEAL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKDtZGKYnPEF",
        "outputId": "a654b722-3d7a-4de6-a75c-aa65ec9b70e8"
      },
      "id": "gKDtZGKYnPEF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting TenSEAL\n",
            "  Downloading tenseal-0.3.16-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Downloading tenseal-0.3.16-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: TenSEAL\n",
            "Successfully installed TenSEAL-0.3.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryFZiQ5U3A9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7ce9f3-e497-4809-812b-638accff492a"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import tenseal as ts\n",
        "except Exception as e:\n",
        "    print('tenseal not available. Install tenseal (pip install tenseal) to use encrypted aggregation.')\n",
        "    ts = None\n"
      ],
      "id": "ryFZiQ5U3A9B",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tenseal not available. Install tenseal (pip install tenseal) to use encrypted aggregation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i7ndMK7kpwQ",
        "outputId": "140e313a-6b27-4499-fcc7-3a5b8a2cf0d0"
      },
      "id": "9i7ndMK7kpwQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aTXyoba3A9D",
        "outputId": "13fddb43-d541-44ed-fdf6-7d89ea14e347"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "SAMPLING_FRACTION = 0.2\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/datasets/CICIDS2017'\n",
        "csv_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n",
        "if not csv_files:\n",
        "    raise FileNotFoundError(f'No CSV files found in {DATA_DIR}. Please upload them and re-run.')\n",
        "\n",
        "print(f'Found {len(csv_files)} CSV files. Reading and concatenating...')\n",
        "dfs = [pd.read_csv(file, low_memory=False) for file in csv_files]\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print('Raw combined shape:', df.shape)\n",
        "\n",
        "\n",
        "if SAMPLING_FRACTION < 1.0:\n",
        "    df_sampled = df.sample(frac=SAMPLING_FRACTION, random_state=42)\n",
        "    print(f'Dataset sampled to {SAMPLING_FRACTION*100:.1f}% of original size.')\n",
        "    print('Sampled shape:', df_sampled.shape)\n",
        "else:\n",
        "    df_sampled = df\n",
        "    print('No sampling applied (SAMPLING_FRACTION is 1.0).')\n",
        "\n",
        "df = df_sampled\n",
        "\n",
        "\n",
        "if 'Label' in df.columns:\n",
        "    label_col = 'Label'\n",
        "elif ' Label' in df.columns:\n",
        "    label_col = ' Label'\n",
        "else:\n",
        "    label_col = df.columns[-1]\n",
        "    print('Using last column as label:', label_col)\n",
        "\n",
        "non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "non_numeric = [c for c in non_numeric if c != label_col]\n",
        "df = df.drop(columns=non_numeric, errors='ignore')\n",
        "\n",
        "unique_labels = df[label_col].unique()\n",
        "label_mapping = {}\n",
        "current_index = 0\n",
        "for lbl in unique_labels:\n",
        "    lbl_str = str(lbl).strip().upper()\n",
        "    if lbl_str.startswith(\"BENIGN\"):\n",
        "        label_mapping[lbl] = 0\n",
        "    else:\n",
        "        current_index += 1\n",
        "        label_mapping[lbl] = current_index\n",
        "\n",
        "df[label_col] = df[label_col].map(label_mapping)\n",
        "df = df.dropna(subset=[label_col])\n",
        "\n",
        "# Feature / label split\n",
        "X = df.drop(columns=[label_col]).values\n",
        "y = df[label_col].values\n",
        "\n",
        "# Replace inf/nan with column means\n",
        "X = np.where(np.isinf(X), np.nan, X)\n",
        "col_means = np.nanmean(X, axis=0)\n",
        "inds = np.where(np.isnan(X))\n",
        "X[inds] = np.take(col_means, inds[1])\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "\n",
        "X_fl_train, X_test_full, y_fl_train, y_test_full = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print('--- Data Summary ---')\n",
        "print(f'Total features shape after sampling/cleaning: {X.shape}')\n",
        "print(f'FL Training Data Shape: {X_fl_train.shape}')\n",
        "print(f'Global Test Data Shape: {X_test_full.shape}')\n",
        "print(f'Number of Classes: {num_classes}')\n",
        "print(\"Fine-grained label distribution (FL Training Set):\")\n",
        "print(pd.Series(y_fl_train).value_counts())\n",
        "print('✅ Preprocessing and Global Split complete.')"
      ],
      "id": "3aTXyoba3A9D",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 CSV files. Reading and concatenating...\n",
            "Raw combined shape: (5349020, 79)\n",
            "Dataset sampled to 20.0% of original size.\n",
            "Sampled shape: (1069804, 79)\n",
            "--- Data Summary ---\n",
            "Total features shape after sampling/cleaning: (1069804, 78)\n",
            "FL Training Data Shape: (855843, 78)\n",
            "Global Test Data Shape: (213961, 78)\n",
            "Number of Classes: 14\n",
            "Fine-grained label distribution (FL Training Set):\n",
            "0     679945\n",
            "2      74108\n",
            "3      50664\n",
            "4      40993\n",
            "5       2521\n",
            "1       1903\n",
            "6       1896\n",
            "9       1746\n",
            "10       720\n",
            "8        646\n",
            "7        491\n",
            "11       194\n",
            "12        11\n",
            "13         5\n",
            "Name: count, dtype: int64\n",
            "✅ Preprocessing and Global Split complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "nW6Nh3rqmNZX"
      },
      "id": "nW6Nh3rqmNZX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U imbalanced-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKXt40-kmoMA",
        "outputId": "63c85175-19b2-4e54-f08c-3113ffdff137"
      },
      "id": "xKXt40-kmoMA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jp5HAoL3A9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1feff656-8bb1-48c4-8c43-0404bb305b8b"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "print(\"Original label distribution (FL Training Set):\", Counter(y_fl_train))\n",
        "\n",
        "\n",
        "TARGET_COUNT = 25000\n",
        "current_counts = Counter(y_fl_train)\n",
        "sampling_strategy = {}\n",
        "\n",
        "for label, count in current_counts.items():\n",
        "\n",
        "    if label != 0:\n",
        "        sampling_strategy[label] = max(count, TARGET_COUNT)\n",
        "    else:\n",
        "\n",
        "        sampling_strategy[label] = count\n",
        "\n",
        "\n",
        "smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=2)\n",
        "X_res, y_res = smote.fit_resample(X_fl_train, y_fl_train)\n",
        "\n",
        "print(\"After CONTROLLED SMOTE on FL Training Data:\")\n",
        "print(Counter(y_res))\n",
        "print(f\"Total samples for FL training after SMOTE: {len(y_res)}\")\n",
        "\n",
        "# Non-IID Split Function (Dirichlet)\n",
        "\n",
        "def split_noniid(X, y, num_clients=4, alpha=0.5):\n",
        "    np.random.seed(42)\n",
        "    labels = np.unique(y)\n",
        "    idx_per_label = {label: np.where(y == label)[0].tolist() for label in labels}\n",
        "    clients_idx = {i: [] for i in range(num_clients)}\n",
        "\n",
        "    for label, idxs in idx_per_label.items():\n",
        "        np.random.shuffle(idxs)\n",
        "        proportions = np.random.dirichlet([alpha]*num_clients)\n",
        "        counts = (proportions * len(idxs)).astype(int)\n",
        "        while counts.sum() < len(idxs):\n",
        "            counts[np.argmax(proportions)] += 1\n",
        "        ptr = 0\n",
        "        for c in range(num_clients):\n",
        "            cnt = counts[c]\n",
        "            if cnt > 0:\n",
        "                clients_idx[c].extend(idxs[ptr:ptr+cnt])\n",
        "                ptr += cnt\n",
        "\n",
        "    clients = []\n",
        "    for c in range(num_clients):\n",
        "        idcs = clients_idx[c]\n",
        "        clients.append((X[idcs], y[idcs]))\n",
        "    return clients\n",
        "\n",
        "num_clients = 4\n",
        "clients = split_noniid(X_res, y_res, num_clients=num_clients, alpha=0.5)\n",
        "\n",
        "print(\"\\n--- Client Data Distribution ---\")\n",
        "for i, (Xc, yc) in enumerate(clients):\n",
        "    print(f\"Client {i+1}: {len(yc)} samples | label distribution: {Counter(yc)}\")\n",
        "print('✅ Client partitioning complete.')"
      ],
      "id": "5Jp5HAoL3A9E",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original label distribution (FL Training Set): Counter({np.int64(0): 679945, np.int64(2): 74108, np.int64(3): 50664, np.int64(4): 40993, np.int64(5): 2521, np.int64(1): 1903, np.int64(6): 1896, np.int64(9): 1746, np.int64(10): 720, np.int64(8): 646, np.int64(7): 491, np.int64(11): 194, np.int64(12): 11, np.int64(13): 5})\n",
            "After CONTROLLED SMOTE on FL Training Data:\n",
            "Counter({np.int64(0): 679945, np.int64(2): 74108, np.int64(3): 50664, np.int64(4): 40993, np.int64(5): 25000, np.int64(10): 25000, np.int64(6): 25000, np.int64(1): 25000, np.int64(8): 25000, np.int64(7): 25000, np.int64(9): 25000, np.int64(11): 25000, np.int64(13): 25000, np.int64(12): 25000})\n",
            "Total samples for FL training after SMOTE: 1095710\n",
            "\n",
            "--- Client Data Distribution ---\n",
            "Client 1: 429055 samples | label distribution: Counter({np.int64(0): 269260, np.int64(3): 41803, np.int64(4): 25235, np.int64(6): 22128, np.int64(13): 18273, np.int64(11): 14639, np.int64(10): 11372, np.int64(1): 11087, np.int64(5): 4985, np.int64(2): 4512, np.int64(9): 4233, np.int64(7): 892, np.int64(8): 360, np.int64(12): 276})\n",
            "Client 2: 96009 samples | label distribution: Counter({np.int64(7): 20051, np.int64(8): 17623, np.int64(12): 17614, np.int64(0): 15867, np.int64(2): 13137, np.int64(1): 4328, np.int64(6): 2214, np.int64(9): 1720, np.int64(11): 1053, np.int64(5): 850, np.int64(10): 691, np.int64(13): 442, np.int64(4): 292, np.int64(3): 127})\n",
            "Client 3: 214230 samples | label distribution: Counter({np.int64(0): 111172, np.int64(2): 53864, np.int64(5): 18184, np.int64(3): 8564, np.int64(4): 8044, np.int64(10): 6895, np.int64(11): 2829, np.int64(13): 1692, np.int64(7): 829, np.int64(12): 636, np.int64(8): 615, np.int64(9): 427, np.int64(6): 252, np.int64(1): 227})\n",
            "Client 4: 356416 samples | label distribution: Counter({np.int64(0): 283646, np.int64(9): 18620, np.int64(1): 9358, np.int64(4): 7422, np.int64(11): 6479, np.int64(12): 6474, np.int64(8): 6402, np.int64(10): 6042, np.int64(13): 4593, np.int64(7): 3228, np.int64(2): 2595, np.int64(5): 981, np.int64(6): 406, np.int64(3): 170})\n",
            "✅ Client partitioning complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGnuKGaq3A9F"
      },
      "source": [
        "\n",
        "class ImprovedMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=512, output_dim=15):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def get_model(input_dim, num_classes):\n",
        "    \"\"\"Returns an initialized ImprovedMLP model with the correct dimensions.\"\"\"\n",
        "\n",
        "    return ImprovedMLP(input_dim, output_dim=num_classes)"
      ],
      "id": "vGnuKGaq3A9F",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpNkU8bw3A9G"
      },
      "source": [
        "def model_to_vector(state_dict):\n",
        "    vec = []\n",
        "    shapes = {}\n",
        "    for k, v in state_dict.items():\n",
        "        arr = v.cpu().numpy().ravel()\n",
        "        shapes[k] = v.shape\n",
        "        vec.append(arr)\n",
        "    flat = np.concatenate(vec).astype(np.float64)\n",
        "    return flat, shapes\n",
        "\n",
        "def vector_to_model(state_dict_template, flat_vec, shapes):\n",
        "    new_state = {}\n",
        "    ptr = 0\n",
        "    for k in state_dict_template.keys():\n",
        "        num = int(np.prod(shapes[k]))\n",
        "        slice_ = flat_vec[ptr:ptr+num]\n",
        "        new_state[k] = torch.tensor(slice_.reshape(shapes[k]), dtype=state_dict_template[k].dtype)\n",
        "        ptr += num\n",
        "    return new_state\n",
        "\n",
        "def create_tenseal_context():\n",
        "    if ts is None:\n",
        "        raise RuntimeError('tenseal not installed')\n",
        "    ctx = ts.context(ts.SCHEME_TYPE.CKKS, poly_modulus_degree=8192, coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 40, 60])\n",
        "    ctx.generate_galois_keys()\n",
        "    ctx.global_scale = 2**40\n",
        "    return ctx\n",
        "\n",
        "def encrypt_vector(ctx, vec):\n",
        "    return ts.ckks_vector(ctx, vec)\n",
        "\n",
        "def decrypt_vector(enc_vec):\n",
        "    return np.array(enc_vec.decrypt())\n"
      ],
      "id": "IpNkU8bw3A9G",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsyujdBu3A9H"
      },
      "source": [
        "def train_local(model, X_train, y_train, epochs=3, batch_size=64, lr=1e-3, device='cpu'):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    for e in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "    return model.state_dict()\n"
      ],
      "id": "QsyujdBu3A9H",
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tenseal as ts\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def federated_train_with_encryption(clients, input_dim, num_classes,\n",
        "                                    rounds=7, local_epochs=5, batch_size=32, device='cpu'):\n",
        "    \"\"\"\n",
        "    Federated Learning with encryption, mini-batching, weighted loss, and improved stability.\n",
        "    \"\"\"\n",
        "    print(f\"Starting Federated Learning with Encryption — Rounds: {rounds}, Clients: {len(clients)}\")\n",
        "\n",
        "-\n",
        "    global_model = get_model(input_dim, num_classes).to(device)\n",
        "\n",
        "\n",
        "    context = ts.context(\n",
        "        ts.SCHEME_TYPE.CKKS,\n",
        "        poly_modulus_degree=16384,\n",
        "        coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 40, 60]\n",
        "    )\n",
        "    context.global_scale = 2**40\n",
        "    context.generate_galois_keys()\n",
        "\n",
        "    for r in range(rounds):\n",
        "        print(f\"\\nRound {r+1}/{rounds} — clients: {len(clients)}\")\n",
        "        encrypted_updates = []\n",
        "        global_weights_dict = global_model.state_dict()\n",
        "\n",
        "\n",
        "        global_weights_vector = torch.cat([p.data.view(-1) for p in global_model.parameters()]).cpu().numpy()\n",
        "\n",
        "        for cid, (Xc, yc) in enumerate(clients):\n",
        "            print(f\" Client {cid}: training on {len(Xc)} samples\")\n",
        "\n",
        "            local_model = get_model(input_dim, num_classes).to(device)\n",
        "            local_model.load_state_dict(global_weights_dict)\n",
        "            optimizer = optim.Adam(local_model.parameters(), lr=0.0001)\n",
        "\n",
        "            class_counts = np.bincount(yc, minlength=num_classes)\n",
        "\n",
        "            weights = 1.0 / (class_counts + 1e-6)\n",
        "            weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "            criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "            dataset = TensorDataset(torch.tensor(Xc, dtype=torch.float32),\n",
        "                                    torch.tensor(yc, dtype=torch.long))\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "            local_model.train()\n",
        "            for epoch in range(local_epochs):\n",
        "                for Xbatch, ybatch in loader:\n",
        "                    Xbatch, ybatch = Xbatch.to(device), ybatch.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    out = local_model(Xbatch)\n",
        "                    loss = criterion(out, ybatch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            local_weights_vector = torch.cat([p.data.view(-1) for p in local_model.parameters()]).cpu().numpy()\n",
        "            model_update = local_weights_vector - global_weights_vector\n",
        "\n",
        "            enc_update_vector = ts.ckks_vector(context, model_update.tolist())\n",
        "            encrypted_updates.append(enc_update_vector)\n",
        "\n",
        "\n",
        "        enc_sum_updates = encrypted_updates[0]\n",
        "        for enc in encrypted_updates[1:]:\n",
        "            # Homomorphic addition\n",
        "            enc_sum_updates += enc\n",
        "\n",
        "        enc_avg_update = enc_sum_updates * (1.0 / len(clients))\n",
        "        decrypted_avg_update = np.array(enc_avg_update.decrypt())\n",
        "        print(\" Decrypted aggregated UPDATE sample (first 10 elements):\", np.round(decrypted_avg_update[:10], 6))\n",
        "\n",
        "        new_global_weights = global_weights_vector + decrypted_avg_update\n",
        "        idx = 0\n",
        "        new_state = {}\n",
        "        for name, param in global_model.state_dict().items():\n",
        "            size = param.numel()\n",
        "            new_state[name] = torch.tensor(new_global_weights[idx:idx+size], dtype=param.dtype).view(param.shape)\n",
        "            idx += size\n",
        "        global_model.load_state_dict(new_state)\n",
        "        print(f\" Round {r+1} global model updated.\")\n",
        "\n",
        "    return global_model"
      ],
      "metadata": {
        "id": "I1Sf5S3nt_iy"
      },
      "id": "I1Sf5S3nt_iy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiqkr-nH3A9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c54f0e-9403-4a8f-8f37-b39933b55b02"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "if 'clients' in globals() and len(clients) >= 1:\n",
        "\n",
        "    input_dim = clients[0][0].shape[1]\n",
        "\n",
        "    try:\n",
        "        global_model = federated_train_with_encryption(\n",
        "            clients, input_dim, num_classes=num_classes, rounds=7, local_epochs=5, device='cpu'\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        print('RuntimeError:', e)\n",
        "        print('If TenSEAL is not installed, you can still run plain aggregation by removing TenSEAL calls.')\n",
        "\n",
        "        global_model = get_model(input_dim, num_classes).to('cpu')\n",
        "\n",
        "    global_model.eval()\n",
        "\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test_full, dtype=torch.float32),\n",
        "                                 torch.tensor(y_test_full, dtype=torch.long))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            out = global_model(xb)\n",
        "            preds = torch.argmax(out, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(yb.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    print('\\n=== Final Global Model Metrics on UNSEEN Test Set ===')\n",
        "    print(f' Accuracy:  {acc:.4f}')\n",
        "    print(f' F1-score:  {f1:.4f}')\n",
        "    print(f' Precision: {prec:.4f}')\n",
        "    print(f' Recall:    {rec:.4f}')\n",
        "\n",
        "    print('\\n=== Per-client evaluation (on local training data) ===')\n",
        "    for cid, (Xc, yc) in enumerate(clients):\n",
        "        client_dataset = TensorDataset(torch.tensor(Xc, dtype=torch.float32),\n",
        "                                       torch.tensor(yc, dtype=torch.long))\n",
        "        client_loader = DataLoader(client_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        client_preds = []\n",
        "        client_labels = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in client_loader:\n",
        "                out = global_model(xb)\n",
        "                preds_c = torch.argmax(out, dim=1)\n",
        "                client_preds.append(preds_c.cpu())\n",
        "                client_labels.append(yb.cpu())\n",
        "\n",
        "        client_preds = torch.cat(client_preds).numpy()\n",
        "        client_labels = torch.cat(client_labels).numpy()\n",
        "\n",
        "        acc_c = accuracy_score(client_labels, client_preds)\n",
        "        f1_c = f1_score(client_labels, client_preds, average='weighted', zero_division=0)\n",
        "        prec_c = precision_score(client_labels, client_preds, average='weighted', zero_division=0)\n",
        "        rec_c = recall_score(client_labels, client_preds, average='weighted', zero_division=0)\n",
        "        print(f' Client {cid} -> Accuracy: {acc_c:.4f}, F1: {f1_c:.4f}, Precision: {prec_c:.4f}, Recall: {rec_c:.4f}')\n",
        "else:\n",
        "    print('Clients not prepared. Ensure dataset was loaded and clients created.')"
      ],
      "id": "jiqkr-nH3A9L",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Federated Learning with Encryption — Rounds: 7, Clients: 4\n",
            "\n",
            "Round 1/7 — clients: 4\n",
            " Client 0: training on 429055 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 1: training on 96009 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 2: training on 214230 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 3: training on 356416 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Decrypted aggregated UPDATE sample (first 10 elements): [ 3.8166e-02 -1.0896e-02 -3.3639e-02 -2.6020e-02 -7.4810e-03  9.4770e-03\n",
            " -2.1751e-02  6.7000e-05 -5.5533e-02 -3.2753e-02]\n",
            " Round 1 global model updated.\n",
            "\n",
            "Round 2/7 — clients: 4\n",
            " Client 0: training on 429055 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 1: training on 96009 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 2: training on 214230 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 3: training on 356416 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Decrypted aggregated UPDATE sample (first 10 elements): [ 0.00326  -0.004124 -0.021566 -0.017041 -0.01186  -0.007758 -0.008129\n",
            " -0.006879 -0.014029 -0.006061]\n",
            " Round 2 global model updated.\n",
            "\n",
            "Round 3/7 — clients: 4\n",
            " Client 0: training on 429055 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 1: training on 96009 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 2: training on 214230 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 3: training on 356416 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Decrypted aggregated UPDATE sample (first 10 elements): [ 0.003403 -0.007114 -0.025332 -0.014679 -0.003966 -0.010377  0.000152\n",
            " -0.010591 -0.004509  0.005633]\n",
            " Round 3 global model updated.\n",
            "\n",
            "Round 4/7 — clients: 4\n",
            " Client 0: training on 429055 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 1: training on 96009 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 2: training on 214230 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 3: training on 356416 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Decrypted aggregated UPDATE sample (first 10 elements): [-0.001832 -0.008331 -0.030389 -0.017703 -0.006689 -0.011553 -0.009837\n",
            " -0.014555 -0.007439 -0.003008]\n",
            " Round 4 global model updated.\n",
            "\n",
            "Round 5/7 — clients: 4\n",
            " Client 0: training on 429055 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 1: training on 96009 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 2: training on 214230 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 3: training on 356416 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Decrypted aggregated UPDATE sample (first 10 elements): [ 0.011612 -0.007485 -0.03047  -0.014267 -0.004459 -0.010941 -0.005343\n",
            " -0.004666 -0.002303 -0.000238]\n",
            " Round 5 global model updated.\n",
            "\n",
            "Round 6/7 — clients: 4\n",
            " Client 0: training on 429055 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 1: training on 96009 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 2: training on 214230 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 3: training on 356416 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Decrypted aggregated UPDATE sample (first 10 elements): [ 0.005306 -0.003266 -0.0177   -0.006939 -0.000728 -0.006619 -0.002515\n",
            " -0.004217  0.001527  0.0048  ]\n",
            " Round 6 global model updated.\n",
            "\n",
            "Round 7/7 — clients: 4\n",
            " Client 0: training on 429055 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 1: training on 96009 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 2: training on 214230 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Client 3: training on 356416 samples\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            " Decrypted aggregated UPDATE sample (first 10 elements): [ 0.003112 -0.014926 -0.03656  -0.016122 -0.006849 -0.01657  -0.001764\n",
            " -0.012232 -0.001545  0.006892]\n",
            " Round 7 global model updated.\n",
            "\n",
            "=== Final Global Model Metrics on UNSEEN Test Set ===\n",
            " Accuracy:  0.9477\n",
            " F1-score:  0.9561\n",
            " Precision: 0.9695\n",
            " Recall:    0.9477\n",
            "\n",
            "=== Per-client evaluation (on local training data) ===\n",
            " Client 0 -> Accuracy: 0.9550, F1: 0.9609, Precision: 0.9719, Recall: 0.9550\n",
            " Client 1 -> Accuracy: 0.8124, F1: 0.8237, Precision: 0.9710, Recall: 0.8124\n",
            " Client 2 -> Accuracy: 0.9607, F1: 0.9629, Precision: 0.9706, Recall: 0.9607\n",
            " Client 3 -> Accuracy: 0.9397, F1: 0.9488, Precision: 0.9701, Recall: 0.9397\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}