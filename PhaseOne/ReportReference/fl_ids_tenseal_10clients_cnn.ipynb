{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "-AohgY_o3A87",
      "metadata": {
        "id": "-AohgY_o3A87"
      },
      "source": [
        "# Federated IDS (10 clients) with TenSEAL (CKKS) encrypted weight aggregation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ryFZiQ5U3A9B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryFZiQ5U3A9B",
        "outputId": "d9cb8c32-60b5-4a75-bb32-26aaf8fb5096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tenseal not available. Install tenseal (pip install tenseal) to use encrypted aggregation.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import tenseal as ts\n",
        "except Exception as e:\n",
        "    print('tenseal not available. Install tenseal (pip install tenseal) to use encrypted aggregation.')\n",
        "    ts = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aTXyoba3A9D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aTXyoba3A9D",
        "outputId": "afefafb6-9952-46a0-e0de-67cccfbdb87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 CSV files. Reading and concatenating...\n",
            "Raw combined shape: (628218, 79)\n",
            "Fine-grained label distribution:\n",
            " Label\n",
            "0    508840\n",
            "1     69636\n",
            "4     48179\n",
            "5      1537\n",
            "3        21\n",
            "2         5\n",
            "Name: count, dtype: int64\n",
            "After cleaning shape: (628218, 79)\n",
            "✅ Features shape: (628218, 78)\n",
            "✅ Labels shape: (628218,)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "DATA_DIR = '/content'\n",
        "\n",
        "csv_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n",
        "if not csv_files:\n",
        "    raise FileNotFoundError(f'No CSV files found in {DATA_DIR}. Please upload them and re-run.')\n",
        "\n",
        "print(f'Found {len(csv_files)} CSV files. Reading and concatenating...')\n",
        "dfs = [pd.read_csv(file, low_memory=False) for file in csv_files]\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print('Raw combined shape:', df.shape)\n",
        "\n",
        "\n",
        "if 'Label' in df.columns:\n",
        "    label_col = 'Label'\n",
        "elif ' Label' in df.columns:\n",
        "    label_col = ' Label'\n",
        "else:\n",
        "    label_col = df.columns[-1]\n",
        "    print(' Using last column as label:', label_col)\n",
        "\n",
        "non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "non_numeric = [c for c in non_numeric if c != label_col]\n",
        "df = df.drop(columns=non_numeric, errors='ignore')\n",
        "\n",
        "unique_labels = df[label_col].unique()\n",
        "label_mapping = {}\n",
        "current_index = 0\n",
        "for lbl in unique_labels:\n",
        "    lbl_str = str(lbl).strip().upper()\n",
        "    if lbl_str.startswith(\"BENIGN\"):\n",
        "        label_mapping[lbl] = 0\n",
        "    else:\n",
        "        current_index += 1\n",
        "        label_mapping[lbl] = current_index\n",
        "\n",
        "df[label_col] = df[label_col].map(label_mapping)\n",
        "\n",
        "print(\"Fine-grained label distribution:\")\n",
        "print(df[label_col].value_counts())\n",
        "\n",
        "df = df.dropna(subset=[label_col])\n",
        "print('After cleaning shape:', df.shape)\n",
        "\n",
        "X = df.drop(columns=[label_col]).values\n",
        "y = df[label_col].values\n",
        "\n",
        "\n",
        "X = np.where(np.isinf(X), np.nan, X)\n",
        "col_means = np.nanmean(X, axis=0)\n",
        "inds = np.where(np.isnan(X))\n",
        "X[inds] = np.take(col_means, inds[1])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "num_classes = len(np.unique(y))\n",
        "print(' Features shape:', X.shape)\n",
        "print(' Labels shape:', y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Jp5HAoL3A9E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jp5HAoL3A9E",
        "outputId": "3137c847-7580-44ca-f364-6aac74fca0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 628218\n",
            "Global training samples (80%): 502574\n",
            "Global test samples (20%): 125644\n",
            "\n",
            "Classes identified as 'splittable' (>= 8 samples): {np.int64(0), np.int64(1), np.int64(3), np.int64(4), np.int64(5)}\n",
            "Classes identified as 'rare' (< 8 samples): {np.int64(2)}\n",
            "Prioritizing clients for local testing: [8, 2, 4, 7]\n",
            "\n",
            "--- Creating Local Train/Test Splits for each client ---\n",
            "Client 1: Total 151644 samples (Too few samples in class 5 (1) to stratify, using all for training)\n",
            "Client 2: Total 23907 samples (Too few samples in class 3 (1) to stratify, using all for training)\n",
            "Client 3: Total 23133 samples -> Train: 18506, Test: 4627 (Local test created)\n",
            "Client 4: Total 214 samples -> Train: 171, Test: 43 (Local test created)\n",
            "Client 5: Total 245929 samples -> Train: 196743, Test: 49186 (Local test created)\n",
            "Client 6: Total 152 samples -> Train: 121, Test: 31 (Local test created)\n",
            "Client 7: Total 1566 samples -> Train: 1252, Test: 314 (Local test created)\n",
            "Client 8: Total 42614 samples -> Train: 34091, Test: 8523 (Local test created)\n",
            "Client 9: Total 349 samples -> Train: 279, Test: 70 (Local test created)\n",
            "Client 10: Total 13066 samples (Too few samples in class 3 (1) to stratify, using all for training)\n",
            "\n",
            "Successfully created data splits for 10 clients.\n",
            "Number of clients with a meaningful local test set: 7\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "global_test_dataset = TensorDataset(torch.tensor(X_test_full, dtype=torch.float32),\n",
        "                                    torch.tensor(y_test_full, dtype=torch.long))\n",
        "global_test_loader = DataLoader(global_test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Global training samples (80%): {len(X_train_full)}\")\n",
        "print(f\"Global test samples (20%): {len(X_test_full)}\")\n",
        "\n",
        "\n",
        "num_clients = 10\n",
        "num_priority_clients = 4\n",
        "min_samples_for_split = 2\n",
        "alpha = 0.1 # Dirichlet parameter\n",
        "\n",
        "unique_classes = np.unique(y_train_full)\n",
        "num_classes = len(unique_classes)\n",
        "class_indices = {cls: np.where(y_train_full == cls)[0] for cls in unique_classes}\n",
        "\n",
        "min_global_samples_needed = num_priority_clients * min_samples_for_split\n",
        "splittable_classes = {cls for cls, indices in class_indices.items() if len(indices) >= min_global_samples_needed}\n",
        "rare_classes = {cls for cls in unique_classes if cls not in splittable_classes}\n",
        "\n",
        "print(f\"\\nClasses identified as 'splittable' (>= {min_global_samples_needed} samples): {splittable_classes}\")\n",
        "print(f\"Classes identified as 'rare' (< {min_global_samples_needed} samples): {rare_classes}\")\n",
        "t\n",
        "client_indices = {i: [] for i in range(num_clients)}\n",
        "assigned_indices_mask = np.zeros(len(X_train_full), dtype=bool)\n",
        "\n",
        "priority_client_ids = random.sample(range(num_clients), num_priority_clients)\n",
        "print(f\"Prioritizing clients for local testing: {priority_client_ids}\")\n",
        "\n",
        "for cls in splittable_classes:\n",
        "    indices_for_class = class_indices[cls]\n",
        "    np.random.shuffle(indices_for_class)\n",
        "\n",
        "    assigned_count = 0\n",
        "    for client_id in priority_client_ids:\n",
        "        # Assign 'min_samples_for_split' indices to this priority client\n",
        "        indices_to_assign = indices_for_class[assigned_count : assigned_count + min_samples_for_split]\n",
        "        client_indices[client_id].extend(indices_to_assign)\n",
        "        assigned_indices_mask[indices_to_assign] = True # Mark these as assigned\n",
        "        assigned_count += min_samples_for_split\n",
        "\n",
        "remaining_global_indices = np.where(~assigned_indices_mask)[0]\n",
        "if len(remaining_global_indices) > 0:\n",
        "    X_remaining = X_train_full[remaining_global_indices]\n",
        "    y_remaining = y_train_full[remaining_global_indices]\n",
        "\n",
        "    remaining_class_indices = {cls: np.where(y_remaining == cls)[0] for cls in unique_classes}\n",
        "\n",
        "    for cls in unique_classes:\n",
        "        cls_remaining_indices = remaining_class_indices.get(cls, np.array([], dtype=int))\n",
        "        if len(cls_remaining_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        np.random.shuffle(cls_remaining_indices)\n",
        "        proportions = np.random.dirichlet(np.ones(num_clients) * alpha)\n",
        "\n",
        "        counts = (proportions * len(cls_remaining_indices)).astype(int)\n",
        "        while counts.sum() < len(cls_remaining_indices):\n",
        "            counts[np.random.choice(num_clients)] += 1\n",
        "\n",
        "        current_idx = 0\n",
        "        for client_id in range(num_clients):\n",
        "            num_samples = counts[client_id]\n",
        "            if num_samples > 0:\n",
        "\n",
        "                original_indices_to_add = remaining_global_indices[cls_remaining_indices[current_idx : current_idx + num_samples]]\n",
        "                client_indices[client_id].extend(original_indices_to_add)\n",
        "                current_idx += num_samples\n",
        "else:\n",
        "     print(\"Warning: All data was assigned during prioritization. No remaining data for Dirichlet distribution.\")\n",
        "\n",
        "clients = []\n",
        "clients_with_local_test = 0\n",
        "\n",
        "print(\"\\n--- Creating Local Train/Test Splits for each client ---\")\n",
        "for i in range(num_clients):\n",
        "\n",
        "    indices = np.unique(np.array(client_indices[i], dtype=int))\n",
        "\n",
        "    if len(indices) == 0:\n",
        "        print(f\"Client {i+1}: Received no data. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    Xc, yc = X_train_full[indices], y_train_full[indices]\n",
        "\n",
        "    label_counts = Counter(yc)\n",
        "    min_class_count = min(label_counts.values()) if label_counts else 0\n",
        "\n",
        "    if min_class_count < min_samples_for_split:\n",
        "        Xc_train, yc_train = Xc, yc\n",
        "        Xc_test, yc_test = Xc[0:1], yc[0:1]\n",
        "        print(f\"Client {i+1}: Total {len(yc)} samples (Too few samples in class {min(label_counts, key=label_counts.get)} ({min_class_count}) to stratify, using all for training)\")\n",
        "    else:\n",
        "\n",
        "        Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "            Xc, yc, test_size=0.2, random_state=42, stratify=yc\n",
        "        )\n",
        "        print(f\"Client {i+1}: Total {len(yc)} samples -> Train: {len(Xc_train)}, Test: {len(Xc_test)} (Local test created)\")\n",
        "        clients_with_local_test += 1\n",
        "\n",
        "    clients.append((Xc_train, yc_train, Xc_test, yc_test))\n",
        "\n",
        "print(f\"\\nSuccessfully created data splits for {len(clients)} clients.\")\n",
        "print(f\"Number of clients with a meaningful local test set: {clients_with_local_test}\")\n",
        "\n",
        "if len(clients) != num_clients:\n",
        "     print(f\"Warning: Expected {num_clients} clients, but only created {len(clients)}.\")\n",
        "if clients_with_local_test < num_priority_clients:\n",
        "     print(f\"Warning: Targeted {num_priority_clients} clients for local testing, but only achieved {clients_with_local_test}. \"\n",
        "           \"This can happen with highly skewed data after the Dirichlet step.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IpNkU8bw3A9G",
      "metadata": {
        "id": "IpNkU8bw3A9G"
      },
      "outputs": [],
      "source": [
        "def model_to_vector(state_dict):\n",
        "    vec = []\n",
        "    shapes = {}\n",
        "    for k, v in state_dict.items():\n",
        "        arr = v.cpu().numpy().ravel()\n",
        "        shapes[k] = v.shape\n",
        "        vec.append(arr)\n",
        "    flat = np.concatenate(vec).astype(np.float64)\n",
        "    return flat, shapes\n",
        "\n",
        "def vector_to_model(state_dict_template, flat_vec, shapes):\n",
        "    new_state = {}\n",
        "    ptr = 0\n",
        "    for k in state_dict_template.keys():\n",
        "        num = int(np.prod(shapes[k]))\n",
        "        slice_ = flat_vec[ptr:ptr+num]\n",
        "        new_state[k] = torch.tensor(slice_.reshape(shapes[k]), dtype=state_dict_template[k].dtype)\n",
        "        ptr += num\n",
        "    return new_state\n",
        "\n",
        "def create_tenseal_context():\n",
        "    if ts is None:\n",
        "        raise RuntimeError('tenseal not installed')\n",
        "    ctx = ts.context(ts.SCHEME_TYPE.CKKS, poly_modulus_degree=8192, coeff_mod_bit_sizes=[60, 40, 40, 60])\n",
        "    ctx.generate_galois_keys()\n",
        "    ctx.global_scale = 2**40\n",
        "    return ctx\n",
        "\n",
        "def encrypt_vector(ctx, vec):\n",
        "    return ts.ckks_vector(ctx, vec)\n",
        "\n",
        "def decrypt_vector(enc_vec):\n",
        "    return np.array(enc_vec.decrypt())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QsyujdBu3A9H",
      "metadata": {
        "id": "QsyujdBu3A9H"
      },
      "outputs": [],
      "source": [
        "def train_local(model, X_train, y_train, epochs=3, batch_size=64, lr=1e-3, device='cpu'):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    for e in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "    return model.state_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1YAc8YJZ3A9H",
      "metadata": {
        "id": "1YAc8YJZ3A9H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tenseal as ts\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from collections import Counter # Added for class counts\n",
        "\n",
        "\n",
        "class IDS1DCNN(nn.Module):\n",
        "    \"\"\"A simple 1D CNN for Intrusion Detection.\"\"\"\n",
        "    def __init__(self, input_dim=78, num_classes=6): # Default input_dim to 78\n",
        "        super(IDS1DCNN, self).__init__()\n",
        "        # Feature extractor with Conv1d layers\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv1d(1, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        features = self.feature_extractor(x)\n",
        "        features_flat = features.view(features.size(0), -1)\n",
        "        logits = self.classifier(features_flat)\n",
        "        return logits\n",
        "\n",
        "def get_model(input_dim, num_classes):\n",
        "    return IDS1DCNN(input_dim, num_classes)\n",
        "\n",
        "def evaluate_model(model, loader, device='cpu'):\n",
        "    \"\"\"Evaluates a PyTorch model on a given DataLoader.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            preds = torch.argmax(out, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(yb.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    return acc, f1, prec, rec\n",
        "#\n",
        "def encrypt_vector_chunks(context, vec, chunk_size=None):\n",
        "    if chunk_size is None:\n",
        "\n",
        "        chunk_size = 8192\n",
        "    enc_chunks = []\n",
        "    for i in range(0, len(vec), chunk_size):\n",
        "        chunk = vec[i:i+chunk_size]\n",
        "        enc_chunks.append(ts.ckks_vector(context, chunk))\n",
        "    return enc_chunks\n",
        "\n",
        "def decrypt_vector_chunks(enc_chunks):\n",
        "    vec = np.concatenate([np.array(c.decrypt()) for c in enc_chunks])\n",
        "    return vec\n",
        "\n",
        "def federated_train_with_encryption(clients, input_dim, num_classes,\n",
        "                                    global_test_loader, # Added argument\n",
        "                                    rounds=3, local_epochs=2, batch_size=64, device='cpu'):\n",
        "\n",
        "    print(f\"Starting Federated Learning with Encryption — Rounds: {rounds}, Clients: {len(clients)}\")\n",
        "\n",
        "    global_model = get_model(input_dim, num_classes).to(device)\n",
        "\n",
        "    context = ts.context(\n",
        "        ts.SCHEME_TYPE.CKKS,\n",
        "        poly_modulus_degree=16384,\n",
        "        coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        "    )\n",
        "    context.global_scale = 2**40\n",
        "    context.generate_galois_keys()\n",
        "\n",
        "    for r in range(rounds):\n",
        "        print(f\"\\nRound {r+1}/{rounds} — clients: {len(clients)}\")\n",
        "        encrypted_updates = []\n",
        "\n",
        "        # LOCAL TRAINING & LOCAL EVALUATION\n",
        "        for cid, (Xc_train, yc_train, Xc_test, yc_test) in enumerate(clients):\n",
        "            print(f\" Client {cid}: training on {len(Xc_train)} samples\")\n",
        "\n",
        "\n",
        "            local_model = get_model(input_dim, num_classes).to(device)\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "            #  Use a lower learning rate for stability, especially with CNNs\n",
        "            optimizer = optim.Adam(local_model.parameters(), lr=0.0001)\n",
        "\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "            train_dataset = TensorDataset(torch.tensor(Xc_train, dtype=torch.float32),\n",
        "                                          torch.tensor(yc_train, dtype=torch.long))\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            local_test_dataset = TensorDataset(torch.tensor(Xc_test, dtype=torch.float32),\n",
        "                                               torch.tensor(yc_test, dtype=torch.long))\n",
        "            local_test_loader = DataLoader(local_test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "            local_model.train()\n",
        "            for epoch in range(local_epochs):\n",
        "                for Xbatch, ybatch in train_loader:\n",
        "                    Xbatch, ybatch = Xbatch.to(device), ybatch.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    out = local_model(Xbatch)\n",
        "                    loss = criterion(out, ybatch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            if len(local_test_loader.dataset) <= 10:\n",
        "                print(f\"  Client {cid}: Local test set too small or unbalanced for meaningful metrics.\")\n",
        "            else:\n",
        "\n",
        "                loc_acc, loc_f1, _, _ = evaluate_model(local_model, local_test_loader, device)\n",
        "                print(f\"  Client {cid} Local Metrics -> Acc: {loc_acc:.4f}, F1: {loc_f1:.4f}\")\n",
        "\n",
        "            weights_vector = torch.cat([p.data.view(-1) for p in local_model.parameters()]).cpu().numpy()\n",
        "            enc_chunks = encrypt_vector_chunks(context, weights_vector)\n",
        "            encrypted_updates.append(enc_chunks)\n",
        "\n",
        "        num_chunks = len(encrypted_updates[0])\n",
        "        aggregated_chunks = []\n",
        "        for chunk_idx in range(num_chunks):\n",
        "            chunk_sum = encrypted_updates[0][chunk_idx]\n",
        "            for client_idx in range(1, len(encrypted_updates)):\n",
        "                chunk_sum += encrypted_updates[client_idx][chunk_idx]\n",
        "            chunk_avg = chunk_sum * (1.0 / len(clients))\n",
        "            aggregated_chunks.append(chunk_avg)\n",
        "\n",
        "        decrypted_avg = decrypt_vector_chunks(aggregated_chunks)\n",
        "        print(\" Decrypted aggregated vector sample (first 10 elements):\", np.round(decrypted_avg[:10], 6))\n",
        "\n",
        "        idx = 0\n",
        "        new_state = {}\n",
        "        for name, param in global_model.state_dict().items():\n",
        "            size = param.numel()\n",
        "            # Ensure tensor shapes match during loading\n",
        "            new_state[name] = torch.tensor(decrypted_avg[idx:idx+size], dtype=param.dtype).view(param.shape)\n",
        "            idx += size\n",
        "        global_model.load_state_dict(new_state)\n",
        "        print(f\" Round {r+1} global model updated.\")\n",
        "\n",
        "        print(f\" Round {r+1}: Evaluating new global model on GLOBAL test set...\")\n",
        "        glob_acc, glob_f1, glob_prec, glob_rec = evaluate_model(global_model, global_test_loader, device)\n",
        "        print(f\" Round {r+1} Global Metrics -> Acc: {glob_acc:.4f}, F1: {glob_f1:.4f}, Prec: {glob_prec:.4f}, Rec: {glob_rec:.4f}\")\n",
        "\n",
        "    return global_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jiqkr-nH3A9L",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiqkr-nH3A9L",
        "outputId": "246bdb61-128f-413c-a86d-2a5189f1fb20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Federated Learning with Encryption — Rounds: 3, Clients: 10\n",
            "\n",
            "Round 1/3 — clients: 10\n",
            " Client 0: training on 151644 samples\n",
            "  Client 0: Local test set too small or unbalanced for meaningful metrics.\n",
            " Client 1: training on 23907 samples\n",
            "  Client 1: Local test set too small or unbalanced for meaningful metrics.\n",
            " Client 2: training on 18506 samples\n",
            "  Client 2 Local Metrics -> Acc: 0.9879, F1: 0.9877\n",
            " Client 3: training on 171 samples\n",
            "  Client 3 Local Metrics -> Acc: 1.0000, F1: 1.0000\n",
            " Client 4: training on 196743 samples\n",
            "  Client 4 Local Metrics -> Acc: 1.0000, F1: 0.9999\n",
            " Client 5: training on 121 samples\n",
            "  Client 5 Local Metrics -> Acc: 1.0000, F1: 1.0000\n",
            " Client 6: training on 1252 samples\n",
            "  Client 6 Local Metrics -> Acc: 0.8854, F1: 0.8822\n",
            " Client 7: training on 34091 samples\n",
            "  Client 7 Local Metrics -> Acc: 0.9885, F1: 0.9884\n",
            " Client 8: training on 279 samples\n",
            "  Client 8 Local Metrics -> Acc: 0.7857, F1: 0.6914\n",
            " Client 9: training on 13066 samples\n",
            "  Client 9: Local test set too small or unbalanced for meaningful metrics.\n",
            " Decrypted aggregated vector sample (first 10 elements): [-0.452903  0.322428  0.538867 -0.578538 -0.019691 -0.138239 -0.547457\n",
            " -0.309786  0.055705  0.209988]\n",
            " Round 1 global model updated.\n",
            " Round 1: Evaluating new global model on GLOBAL test set...\n",
            " Round 1 Global Metrics -> Acc: 0.8859, F1: 0.8468, Prec: 0.8207, Rec: 0.8859\n",
            "\n",
            "Round 2/3 — clients: 10\n",
            " Client 0: training on 151644 samples\n",
            "  Client 0: Local test set too small or unbalanced for meaningful metrics.\n",
            " Client 1: training on 23907 samples\n",
            "  Client 1: Local test set too small or unbalanced for meaningful metrics.\n",
            " Client 2: training on 18506 samples\n",
            "  Client 2 Local Metrics -> Acc: 0.9825, F1: 0.9823\n",
            " Client 3: training on 171 samples\n",
            "  Client 3 Local Metrics -> Acc: 1.0000, F1: 1.0000\n",
            " Client 4: training on 196743 samples\n",
            "  Client 4 Local Metrics -> Acc: 0.9999, F1: 0.9999\n",
            " Client 5: training on 121 samples\n",
            "  Client 5 Local Metrics -> Acc: 0.0000, F1: 0.0000\n",
            " Client 6: training on 1252 samples\n",
            "  Client 6 Local Metrics -> Acc: 0.8758, F1: 0.8722\n",
            " Client 7: training on 34091 samples\n",
            "  Client 7 Local Metrics -> Acc: 0.9885, F1: 0.9884\n",
            " Client 8: training on 279 samples\n",
            "  Client 8 Local Metrics -> Acc: 0.7857, F1: 0.6914\n",
            " Client 9: training on 13066 samples\n",
            "  Client 9: Local test set too small or unbalanced for meaningful metrics.\n",
            " Decrypted aggregated vector sample (first 10 elements): [-0.448939  0.319099  0.543625 -0.590368 -0.028937 -0.149623 -0.553819\n",
            " -0.317498  0.057081  0.212114]\n",
            " Round 2 global model updated.\n",
            " Round 2: Evaluating new global model on GLOBAL test set...\n",
            " Round 2 Global Metrics -> Acc: 0.9146, F1: 0.8771, Prec: 0.8434, Rec: 0.9146\n",
            "\n",
            "Round 3/3 — clients: 10\n",
            " Client 0: training on 151644 samples\n",
            "  Client 0: Local test set too small or unbalanced for meaningful metrics.\n",
            " Client 1: training on 23907 samples\n",
            "  Client 1: Local test set too small or unbalanced for meaningful metrics.\n",
            " Client 2: training on 18506 samples\n",
            "  Client 2 Local Metrics -> Acc: 0.9857, F1: 0.9856\n",
            " Client 3: training on 171 samples\n",
            "  Client 3 Local Metrics -> Acc: 1.0000, F1: 1.0000\n",
            " Client 4: training on 196743 samples\n",
            "  Client 4 Local Metrics -> Acc: 1.0000, F1: 0.9999\n",
            " Client 5: training on 121 samples\n",
            "  Client 5 Local Metrics -> Acc: 0.0000, F1: 0.0000\n",
            " Client 6: training on 1252 samples\n",
            "  Client 6 Local Metrics -> Acc: 0.9045, F1: 0.9019\n",
            " Client 7: training on 34091 samples\n",
            "  Client 7 Local Metrics -> Acc: 0.9939, F1: 0.9939\n",
            " Client 8: training on 279 samples\n",
            "  Client 8 Local Metrics -> Acc: 0.9143, F1: 0.9048\n",
            " Client 9: training on 13066 samples\n",
            "  Client 9: Local test set too small or unbalanced for meaningful metrics.\n",
            " Decrypted aggregated vector sample (first 10 elements): [-0.445983  0.318987  0.548021 -0.602226 -0.037764 -0.160123 -0.558221\n",
            " -0.32179   0.058689  0.215189]\n",
            " Round 3 global model updated.\n",
            " Round 3: Evaluating new global model on GLOBAL test set...\n",
            " Round 3 Global Metrics -> Acc: 0.9857, F1: 0.9845, Prec: 0.9833, Rec: 0.9857\n",
            "\n",
            "\n",
            "=== FEDERATED TRAINING COMPLETE ===\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torch\n",
        "\n",
        "if 'clients' in globals() and len(clients) >= 1:\n",
        "\n",
        "    input_dim = clients[0][0].shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "    try:\n",
        "        global_model = federated_train_with_encryption(\n",
        "            clients,\n",
        "            input_dim,\n",
        "            num_classes=num_classes,\n",
        "            global_test_loader=global_test_loader, # <-- Pass the global test loader\n",
        "            rounds=3,\n",
        "            local_epochs=2,\n",
        "            device='cpu'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print('An error occurred during training:', e)\n",
        "        raise e\n",
        "\n",
        "    print(\"\\n\\n=== FEDERATED TRAINING COMPLETE ===\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print('Clients not prepared. Ensure dataset was loaded and clients created in Cell 5.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
